{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bW9q8qD_bPhY"
      },
      "source": [
        "**Locally Hosted Semantic Reranker**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf09mgBb5Bb9"
      },
      "source": [
        "# Objectives\n",
        "\n",
        "In this notebook we will:\n",
        "- Load a semantic reranker into Elasticsearch with Eland\n",
        "- Create a reranker inference API\n",
        "- Modify the query to use the reranker as part of the query to gather contextual documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BecBOzyDbWik"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Here we do the following\n",
        "- Import the required libraries\n",
        "- Create an elasticsearch python client connection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ayhDP72bZAe"
      },
      "source": [
        "These should already be installed in your notebook environment.\n",
        "You can uncomment and run if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2Xz9uWQFbNkH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
            "You should consider upgrading via the '/Users/rajeshmenon/topramen/onboarding-search-1/.venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "zsh:1: no matches found: eland[pytorch]\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -qU elasticsearch\n",
        "%pip install -qU eland[pytorch]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgHQaJh0bmJQ"
      },
      "source": [
        "Import the required python libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CsL466H0bjNX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from elasticsearch import Elasticsearch, helpers, exceptions\n",
        "from urllib.request import urlopen\n",
        "from getpass import getpass\n",
        "import json\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsQ4XIpkbpd4"
      },
      "source": [
        "Create an Elasticsearch Python client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "P8Pf-lDepKdv"
      },
      "outputs": [],
      "source": [
        "elastic_endpoint = os.getenv('ELASTIC_ENDPOINT')\n",
        "elastic_api_key = os.getenv('ELASTIC_API_KEY')\n",
        "\n",
        "es = Elasticsearch(\n",
        "    elastic_endpoint,\n",
        "    api_key=elastic_api_key,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bsLLnqCfNKk"
      },
      "source": [
        "# Upload Hugging Face model with Eland\n",
        "Here, we will:\n",
        "- Upload the model from Hugging Face to Elasticsearch\n",
        "- Use Eland's `eland_import_hub_model` command to upload the model to Elasticsearch.\n",
        "\n",
        "For this example we've chosen the [`cross-encoder/ms-marco-MiniLM-L-6-v2`](https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-6-v2) text similarity model.\n",
        "<br><br>\n",
        "**Note**:\n",
        "While we are importing the model for use as a reranker, Eland and Elasticsearch do not have a dedicated rerank task type, so we still use `text_similarity`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "J2MTEYrUfk9R"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"cohere/rerank-english-v2.0\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "!eland_import_hub_model \\\n",
        "  --url $elastic_endpoint \\\n",
        "  --es-api-key $elastic_api_key \\\n",
        "  --hub-model-id $MODEL_ID \\\n",
        "  --task-type text_similarity \\\n",
        "  --start \\\n",
        "  --clear-previous\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rrQV6SAgWz8"
      },
      "source": [
        "# Create Inference Endpoint\n",
        "Here we will:\n",
        "- Create an inference Endpoint\n",
        "- Deploy the reranking model we impoted in the previous section\n",
        "We need to create an endpoint queries can use for reranking\n",
        "\n",
        "Key points about the `model_config`\n",
        "- `service` - in this case `elasticsearch` will tell the inference API to use a locally hosted (in Elasticsearch) model\n",
        "- `num_allocations` sets the number of allocations to 1\n",
        "    - Allocations are independent units of work for NLP tasks. Scaling this allows for an increase in concurrent throughput\n",
        "- `num_threads` - sets the number of threads per allocation to 1\n",
        "    - Threads per allocation affect the number of threads used by each allocation during inference. Scaling this generally increased the speed of inference requests (to a point).\n",
        "- `model_id` - This is the id of the model as it is named in Elasticsearch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Abu084BYgWCE"
      },
      "outputs": [],
      "source": [
        "model_config = {\n",
        "  \"service\": \"elasticsearch\",\n",
        "  \"service_settings\": {\n",
        "    \"num_allocations\": 1,\n",
        "    \"num_threads\": 1,\n",
        "    \"model_id\": \"cross-encoder__ms-marco-minilm-l-6-v2\"\n",
        "  },\n",
        "      \"task_settings\": {\n",
        "        \"return_documents\": True\n",
        "    }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'inference_id': 'semantic-reranking',\n",
              " 'task_type': 'rerank',\n",
              " 'service': 'elasticsearch',\n",
              " 'service_settings': {'num_allocations': 1,\n",
              "  'num_threads': 1,\n",
              "  'model_id': 'cross-encoder__ms-marco-minilm-l-6-v2'},\n",
              " 'task_settings': {'return_documents': True}}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "inference_id = \"semantic-reranking\"\n",
        "\n",
        "create_endpoint = es.inference.put(\n",
        "    inference_id=inference_id,\n",
        "    task_type=\"rerank\",\n",
        "    body=model_config\n",
        ")\n",
        "\n",
        "create_endpoint.body"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8rQXMrHhMkS"
      },
      "source": [
        "###Verify it was created\n",
        "\n",
        "- Run the two cells in this section to verify:\n",
        "- The Inference Endpoint has been completed\n",
        "- The model has been deployed\n",
        "\n",
        "You should see JSON output with information about the semantic endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "n3Yk7rgYhP-N"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'endpoints': [{'inference_id': 'semantic-reranking',\n",
              "   'task_type': 'rerank',\n",
              "   'service': 'elasticsearch',\n",
              "   'service_settings': {'num_allocations': 1,\n",
              "    'num_threads': 1,\n",
              "    'model_id': 'cross-encoder__ms-marco-minilm-l-6-v2'},\n",
              "   'task_settings': {'return_documents': True}}]}"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "check_endpoint = es.inference.get(\n",
        "    inference_id=inference_id,\n",
        ")\n",
        "\n",
        "check_endpoint.body"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dqYN5B4gI7v"
      },
      "source": [
        "Verify the model was successfully deployed\n",
        "\n",
        "The cell below should return `started`\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "tui0K4JIgNmf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'model_id': 'cross-encoder__ms-marco-minilm-l-6-v2', 'model_size_stats': {'model_size_bytes': 90892372, 'required_native_memory_bytes': 1229655976}, 'pipeline_count': 0, 'inference_stats': {'failure_count': 0, 'inference_count': 0, 'cache_miss_count': 0, 'missing_all_fields_count': 0, 'timestamp': 1728062554997}, 'deployment_stats': {'deployment_id': 'semantic-reranking', 'model_id': 'cross-encoder__ms-marco-minilm-l-6-v2', 'threads_per_allocation': 1, 'number_of_allocations': 1, 'queue_capacity': 1024, 'state': 'starting', 'reason': 'Could not assign (more) allocations on node [HyWdEyhFS1Wp5gd5Ih4CKg]. Reason: This node has insufficient allocated processors. Available processors [32], free processors [0], processors required for each allocation of this model [1]', 'allocation_status': {'allocation_count': 0, 'target_allocation_count': 1, 'state': 'starting'}, 'cache_size': '86.6mb', 'priority': 'normal', 'start_time': 1728059862522, 'peak_throughput_per_minute': 0, 'nodes': []}}, {'model_id': 'cross-encoder__ms-marco-minilm-l-6-v2', 'model_size_stats': {'model_size_bytes': 90892372, 'required_native_memory_bytes': 1229655976}, 'pipeline_count': 0, 'inference_stats': {'failure_count': 0, 'inference_count': 0, 'cache_miss_count': 0, 'missing_all_fields_count': 0, 'timestamp': 1728062554997}, 'deployment_stats': {'deployment_id': 'cross-encoder__ms-marco-minilm-l-6-v2', 'model_id': 'cross-encoder__ms-marco-minilm-l-6-v2', 'threads_per_allocation': 8, 'number_of_allocations': 2, 'queue_capacity': 1024, 'state': 'started', 'reason': 'Could not assign (more) allocations on node [HyWdEyhFS1Wp5gd5Ih4CKg]. Reason: This node has insufficient allocated processors. Available processors [32], free processors [0], processors required for each allocation of this model [8]', 'allocation_status': {'allocation_count': 1, 'target_allocation_count': 2, 'state': 'started'}, 'cache_size': '86.6mb', 'priority': 'normal', 'start_time': 1727995750890, 'peak_throughput_per_minute': 0, 'nodes': [{'node': {'HyWdEyhFS1Wp5gd5Ih4CKg': {'name': 'instance-0000000005', 'ephemeral_id': 'Z-WKgUucQG6VYCt3WbmKuQ', 'transport_address': '10.42.14.113:19833', 'external_id': 'instance-0000000005', 'attributes': {'xpack.installed': 'true', 'transform.config_version': '10.0.0', 'server_name': 'instance-0000000005.448b18343d5f44fb9cb98a9ace2068f3', 'availability_zone': 'us-central1-a', 'ml.config_version': '12.0.0', 'logical_availability_zone': 'zone-0', 'instance_configuration': 'gcp.es.ml.n2.68x32x45', 'ml.allocated_processors_double': '32.0', 'ml.machine_memory': '68028465152', 'ml.max_jvm_size': '11953766400', 'ml.allocated_processors': '32', 'region': 'unknown-region'}, 'roles': ['ml', 'remote_cluster_client'], 'version': '8.15.0', 'min_index_version': 7000099, 'max_index_version': 8512000}}, 'routing_state': {'routing_state': 'started'}, 'inference_count': 0, 'inference_cache_hit_count': 0, 'number_of_pending_requests': 0, 'start_time': 1727995773592, 'threads_per_allocation': 8, 'number_of_allocations': 1, 'peak_throughput_per_minute': 0, 'throughput_last_minute': 0, 'inference_cache_hit_count_last_minute': 0}]}}]\n"
          ]
        }
      ],
      "source": [
        "ES_MODEL_ID = \"cross-encoder__ms-marco-minilm-l-6-v2\"\n",
        "\n",
        "model_info = es.ml.get_trained_models_stats(model_id=ES_MODEL_ID)\n",
        "\n",
        "print(model_info.body['trained_model_stats'])\n",
        "# model_info.body['trained_model_stats'][0]['deployment_stats']['nodes'][0]['routing_state']['routing_state']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bwvzLfRjJ2n"
      },
      "source": [
        "# Query with Reranking\n",
        "\n",
        "This containes a `text_similarity_reranker` retriever which:\n",
        "1. Uses a Standard Retriever to :\n",
        "    1. Perform a semantic query against the chunked ELSER embeddings\n",
        "    2. Return the top 2 inner hit chunks\n",
        "2. Perform a reranking:\n",
        "    1. Taks as input the top 50 results from the previous search\n",
        "      - `\"rank_window_size\": 50`\n",
        "    2. Taks as input the uer's question\n",
        "      - `\"inference_text\": USER_QUESTION`\n",
        "    3.  Uses our previously created reranking API and model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "HWXQBS35jQ3n"
      },
      "outputs": [
        {
          "ename": "BadRequestError",
          "evalue": "BadRequestError(400, 'x_content_parse_exception', 'unknown query [text_similarity_reranker]')",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[55], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m USER_QUESTION \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLooking back and connecting the dots\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myoutube_subtitles\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbool\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmust\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnested\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m              \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext_semantic.inference.chunks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m              \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mknn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                  \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfield\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext_semantic.inference.chunks.embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                  \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery_vector_builder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext_embedding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmy-elser-endpoint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_text\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mUSER_QUESTION\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                  \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                  \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mk\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m              \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m              \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minner_hits\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_source\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext_semantic.inference.chunks.text\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m              \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m          \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m      \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrescore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwindow_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrescore_query\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext_similarity_reranker\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m              \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minference_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msemantic-reranking\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m              \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfield\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m              \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minference_text\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mUSER_QUESTION\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m          \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m      \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m response\u001b[38;5;241m.\u001b[39mraw\n",
            "File \u001b[0;32m~/topramen/onboarding-search-1/.venv/lib/python3.9/site-packages/elasticsearch/_sync/client/utils.py:446\u001b[0m, in \u001b[0;36m_rewrite_parameters.<locals>.wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    444\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapi\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/topramen/onboarding-search-1/.venv/lib/python3.9/site-packages/elasticsearch/_sync/client/__init__.py:4149\u001b[0m, in \u001b[0;36msearch\u001b[0;34m(self, index, aggregations, aggs, allow_no_indices, allow_partial_search_results, analyze_wildcard, analyzer, batched_reduce_size, ccs_minimize_roundtrips, collapse, default_operator, df, docvalue_fields, error_trace, expand_wildcards, explain, ext, fields, filter_path, force_synthetic_source, from_, highlight, human, ignore_throttled, ignore_unavailable, include_named_queries_score, indices_boost, knn, lenient, max_concurrent_shard_requests, min_compatible_shard_node, min_score, pit, post_filter, pre_filter_shard_size, preference, pretty, profile, q, query, rank, request_cache, rescore, rest_total_hits_as_int, retriever, routing, runtime_mappings, script_fields, scroll, search_after, search_type, seq_no_primary_term, size, slice, sort, source, source_excludes, source_includes, stats, stored_fields, suggest, suggest_field, suggest_mode, suggest_size, suggest_text, terminate_after, timeout, track_scores, track_total_hits, typed_keys, version, body)\u001b[0m\n\u001b[1;32m   4118\u001b[0m         __headers[\"content-type\"] = \"application/json\"\n\u001b[1;32m   4119\u001b[0m     return self.perform_request(  # type: ignore[return-value]\n\u001b[1;32m   4120\u001b[0m         \"POST\",\n\u001b[1;32m   4121\u001b[0m         __path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4126\u001b[0m         path_parts=__path_parts,\n\u001b[1;32m   4127\u001b[0m     )\n\u001b[1;32m   4129\u001b[0m @_rewrite_parameters(\n\u001b[1;32m   4130\u001b[0m     body_fields=(\n\u001b[1;32m   4131\u001b[0m         \"aggs\",\n\u001b[1;32m   4132\u001b[0m         \"buffer\",\n\u001b[1;32m   4133\u001b[0m         \"exact_bounds\",\n\u001b[1;32m   4134\u001b[0m         \"extent\",\n\u001b[1;32m   4135\u001b[0m         \"fields\",\n\u001b[1;32m   4136\u001b[0m         \"grid_agg\",\n\u001b[1;32m   4137\u001b[0m         \"grid_precision\",\n\u001b[1;32m   4138\u001b[0m         \"grid_type\",\n\u001b[1;32m   4139\u001b[0m         \"query\",\n\u001b[1;32m   4140\u001b[0m         \"runtime_mappings\",\n\u001b[1;32m   4141\u001b[0m         \"size\",\n\u001b[1;32m   4142\u001b[0m         \"sort\",\n\u001b[1;32m   4143\u001b[0m         \"track_total_hits\",\n\u001b[1;32m   4144\u001b[0m         \"with_labels\",\n\u001b[1;32m   4145\u001b[0m     ),\n\u001b[1;32m   4146\u001b[0m )\n\u001b[1;32m   4147\u001b[0m def search_mvt(\n\u001b[1;32m   4148\u001b[0m     self,\n\u001b[0;32m-> 4149\u001b[0m     *,\n\u001b[1;32m   4150\u001b[0m     index: t.Union[str, t.Sequence[str]],\n\u001b[1;32m   4151\u001b[0m     field: str,\n\u001b[1;32m   4152\u001b[0m     zoom: int,\n\u001b[1;32m   4153\u001b[0m     x: int,\n\u001b[1;32m   4154\u001b[0m     y: int,\n\u001b[1;32m   4155\u001b[0m     aggs: t.Optional[t.Mapping[str, t.Mapping[str, t.Any]]] = None,\n\u001b[1;32m   4156\u001b[0m     buffer: t.Optional[int] = None,\n\u001b[1;32m   4157\u001b[0m     error_trace: t.Optional[bool] = None,\n\u001b[1;32m   4158\u001b[0m     exact_bounds: t.Optional[bool] = None,\n\u001b[1;32m   4159\u001b[0m     extent: t.Optional[int] = None,\n\u001b[1;32m   4160\u001b[0m     fields: t.Optional[t.Union[str, t.Sequence[str]]] = None,\n\u001b[1;32m   4161\u001b[0m     filter_path: t.Optional[t.Union[str, t.Sequence[str]]] = None,\n\u001b[1;32m   4162\u001b[0m     grid_agg: t.Optional[t.Union[\"t.Literal['geohex', 'geotile']\", str]] = None,\n\u001b[1;32m   4163\u001b[0m     grid_precision: t.Optional[int] = None,\n\u001b[1;32m   4164\u001b[0m     grid_type: t.Optional[\n\u001b[1;32m   4165\u001b[0m         t.Union[\"t.Literal['centroid', 'grid', 'point']\", str]\n\u001b[1;32m   4166\u001b[0m     ] = None,\n\u001b[1;32m   4167\u001b[0m     human: t.Optional[bool] = None,\n\u001b[1;32m   4168\u001b[0m     pretty: t.Optional[bool] = None,\n\u001b[1;32m   4169\u001b[0m     query: t.Optional[t.Mapping[str, t.Any]] = None,\n\u001b[1;32m   4170\u001b[0m     runtime_mappings: t.Optional[t.Mapping[str, t.Mapping[str, t.Any]]] = None,\n\u001b[1;32m   4171\u001b[0m     size: t.Optional[int] = None,\n\u001b[1;32m   4172\u001b[0m     sort: t.Optional[\n\u001b[1;32m   4173\u001b[0m         t.Union[\n\u001b[1;32m   4174\u001b[0m             t.Sequence[t.Union[str, t.Mapping[str, t.Any]]],\n\u001b[1;32m   4175\u001b[0m             t.Union[str, t.Mapping[str, t.Any]],\n\u001b[1;32m   4176\u001b[0m         ]\n\u001b[1;32m   4177\u001b[0m     ] = None,\n\u001b[1;32m   4178\u001b[0m     track_total_hits: t.Optional[t.Union[bool, int]] = None,\n\u001b[1;32m   4179\u001b[0m     with_labels: t.Optional[bool] = None,\n\u001b[1;32m   4180\u001b[0m     body: t.Optional[t.Dict[str, t.Any]] = None,\n\u001b[1;32m   4181\u001b[0m ) -> BinaryApiResponse:\n\u001b[1;32m   4182\u001b[0m     \"\"\"\n\u001b[1;32m   4183\u001b[0m     Searches a vector tile for geospatial values. Returns results as a binary Mapbox\n\u001b[1;32m   4184\u001b[0m     vector tile.\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4229\u001b[0m         point features representing suggested label positions for the original features.\n\u001b[1;32m   4230\u001b[0m     \"\"\"\n\u001b[1;32m   4231\u001b[0m     if index in SKIP_IN_PATH:\n",
            "File \u001b[0;32m~/topramen/onboarding-search-1/.venv/lib/python3.9/site-packages/elasticsearch/_sync/client/_base.py:271\u001b[0m, in \u001b[0;36mBaseClient.perform_request\u001b[0;34m(self, method, path, params, headers, body, endpoint_id, path_parts)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mperform_request\u001b[39m(\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    257\u001b[0m     method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    264\u001b[0m     path_parts: Optional[Mapping[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    265\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ApiResponse[Any]:\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_otel\u001b[38;5;241m.\u001b[39mspan(\n\u001b[1;32m    267\u001b[0m         method,\n\u001b[1;32m    268\u001b[0m         endpoint_id\u001b[38;5;241m=\u001b[39mendpoint_id,\n\u001b[1;32m    269\u001b[0m         path_parts\u001b[38;5;241m=\u001b[39mpath_parts \u001b[38;5;129;01mor\u001b[39;00m {},\n\u001b[1;32m    270\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m otel_span:\n\u001b[0;32m--> 271\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_perform_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m            \u001b[49m\u001b[43motel_span\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43motel_span\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m         otel_span\u001b[38;5;241m.\u001b[39mset_elastic_cloud_metadata(response\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mheaders)\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "File \u001b[0;32m~/topramen/onboarding-search-1/.venv/lib/python3.9/site-packages/elasticsearch/_sync/client/_base.py:352\u001b[0m, in \u001b[0;36mBaseClient._perform_request\u001b[0;34m(self, method, path, params, headers, body, otel_span)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mKeyError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m    350\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 352\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTP_EXCEPTIONS\u001b[38;5;241m.\u001b[39mget(meta\u001b[38;5;241m.\u001b[39mstatus, ApiError)(\n\u001b[1;32m    353\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage, meta\u001b[38;5;241m=\u001b[39mmeta, body\u001b[38;5;241m=\u001b[39mresp_body\n\u001b[1;32m    354\u001b[0m     )\n\u001b[1;32m    356\u001b[0m \u001b[38;5;66;03m# 'X-Elastic-Product: Elasticsearch' should be on every 2XX response.\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verified_elasticsearch:\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;66;03m# If the header is set we mark the server as verified.\u001b[39;00m\n",
            "\u001b[0;31mBadRequestError\u001b[0m: BadRequestError(400, 'x_content_parse_exception', 'unknown query [text_similarity_reranker]')"
          ]
        }
      ],
      "source": [
        "USER_QUESTION = \"Looking back and connecting the dots\"\n",
        "\n",
        "response = es.search(\n",
        "    index=\"youtube_subtitles\",\n",
        "    body={\n",
        "      \"query\": {\n",
        "        \"bool\": {\n",
        "          \"must\": {\n",
        "            \"nested\": {\n",
        "              \"path\": \"text_semantic.inference.chunks\",\n",
        "              \"query\": {\n",
        "                \"knn\": {\n",
        "                  \"field\": \"text_semantic.inference.chunks.embeddings\",\n",
        "                  \"query_vector_builder\": {\n",
        "                    \"text_embedding\": {\n",
        "                      \"model_id\": \"my-elser-endpoint\",\n",
        "                      \"model_text\": USER_QUESTION\n",
        "                    }\n",
        "                  },\n",
        "                  \"k\": 10\n",
        "                }\n",
        "              },\n",
        "              \"inner_hits\": {\n",
        "                \"size\": 2,\n",
        "                \"_source\": [\"text_semantic.inference.chunks.text\"]\n",
        "              }\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      },\n",
        "      \"rescore\": {\n",
        "        \"window_size\": 50,\n",
        "        \"query\": {\n",
        "          \"rescore_query\": {\n",
        "            \"text_similarity_reranker\": {\n",
        "              \"inference_id\": \"semantic-reranking\",\n",
        "              \"field\": \"text\",\n",
        "              \"inference_text\": USER_QUESTION\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        ")\n",
        "\n",
        "response.raw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0HyNZoWyeun"
      },
      "source": [
        "Print out the formatted response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZEx-46rn3in"
      },
      "outputs": [],
      "source": [
        "for review in response.raw['hits']['hits']:\n",
        "    print(f\"Restaurant {review['_source']['Restaurant']} - Rating: {review['_source']['Rating']} - Reviewer: {review['_source']['Reviewer']}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
